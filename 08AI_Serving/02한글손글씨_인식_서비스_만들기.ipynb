{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d822d036",
   "metadata": {},
   "source": [
    "# 한글 손글씨 인식 서비스 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "669397dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, AutoTokenizer\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, AutoTokenizer\n",
    "import requests \n",
    "import unicodedata\n",
    "from io import BytesIO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b48e68e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8050af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# load image from the IAM database\n",
    "url = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\n",
    "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\n",
    "pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "generated_ids = model.generate(pixel_values)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddd754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import VisionEncoderDecoderModel\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"team-lucid/trocr-small-korean\")\n",
    "\n",
    "pixel_values = torch.rand(1, 3, 384, 384)\n",
    "generated_ids = model.generate(pixel_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240d7f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, AutoTokenizer\n",
    "import requests \n",
    "import unicodedata\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"ddobokki/ko-trocr\") \n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"ddobokki/ko-trocr\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ddobokki/ko-trocr\")\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/ddobokki/ocr_img_example/master/g.jpg\"\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "pixel_values = processor(img, return_tensors=\"pt\").pixel_values \n",
    "generated_ids = model.generate(pixel_values, max_length=64)\n",
    "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "generated_text = unicodedata.normalize(\"NFC\", generated_text)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57975d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] init TrOCR Inferencer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniforge3/envs/torch/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7871\n",
      "IMPORTANT: You are using gradio version 3.40.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n",
      "Running on public URL: https://354061f7d5e6fb531c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TrOCRInferencer:\n",
    "    def __init__(self):\n",
    "        print(\"[info] init TrOCR Inferencer\")\n",
    "        self.processor = TrOCRProcessor.from_pretrained(\"ddobokki/ko-trocr\")\n",
    "        self.model = VisionEncoderDecoderModel.from_pretrained(\"ddobokki/ko-trocr\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"ddobokki/ko-trocr\")\n",
    "    def inference(self, image):\n",
    "        pixel_values = self.processor(images=image, return_tensors='pt').pixel_values\n",
    "        generated_ids = self.model.generate(pixel_values, max_length=64)\n",
    "        generated_text = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        generated_text = unicodedata.normalize(\"NFC\", generated_text)\n",
    "        \n",
    "        return generated_text\n",
    "\n",
    "inferencer = TrOCRInferencer()\n",
    "\n",
    "def image_to_text(image):\n",
    "    image = Image.fromarray(image).convert('RGB')\n",
    "    text = inferencer.inference(image)\n",
    "    return text\n",
    "with gr.Blocks() as app:\n",
    "    gr.Markdown(\"# Handwritten Image OCR\")\n",
    "    with gr.Tab(\"Image upload\"):\n",
    "        image = gr.Image(label=\"Handritten image file\")\n",
    "        output = gr.Textbox(label=\"Output Box\")\n",
    "        convert_btn = gr.Button(\"Convert\")\n",
    "        convert_btn.click(\n",
    "            fn=image_to_text, inputs=image, outputs=output\n",
    "        )\n",
    "        gr.Markdown(\"## Image Examples\")\n",
    "        gr.Examples(\n",
    "            examples=[\n",
    "                os.path.join(os.getcwd(), \"examples/Hello.png\"),\n",
    "                os.path.join(os.getcwd(), \"examples/Hello_cursive.png\"),\n",
    "                os.path.join(os.getcwd(), \"examples/Red.png\"),\n",
    "                os.path.join(os.getcwd(), \"examples/sentence.png\"),\n",
    "                os.path.join(os.getcwd(), \"examples/i_love_you.png\"),\n",
    "                os.path.join(os.getcwd(), \"examples/merrychristmas.png\"),\n",
    "                os.path.join(os.getcwd(), \"examples/Rock.png\"),\n",
    "                os.path.join(os.getcwd(), \"examples/Bob.png\"),\n",
    "                ],\n",
    "            inputs=image,\n",
    "            outputs=output,\n",
    "            fn=image_to_text\n",
    "            )\n",
    "    with gr.Tab(\"Drawing\"):\n",
    "        gr.Markdown(\"# Handwritten Image OCR\")\n",
    "        sketchpad = gr.Sketchpad(\n",
    "            label = \"Handwritten Sektchpad\",\n",
    "            shape=(600, 300),\n",
    "            brush_radius=3,\n",
    "            invert_colors=False,\n",
    "            )\n",
    "        output = gr.Textbox(label=\"Output Box\")\n",
    "        convert_btn = gr.Button(\"Convert\")\n",
    "        convert_btn.click(\n",
    "            fn=image_to_text, inputs=sketchpad, outputs=output\n",
    "            )\n",
    "app.launch(inline=False, share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2420192d",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc772452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniforge3/envs/torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/user/miniforge3/envs/torch/lib/python3.11/site-packages/gradio_client/documentation.py:106: UserWarning: Could not get documentation group for <class 'gradio.mix.Parallel'>: No known documentation group for module 'gradio.mix'\n",
      "  warnings.warn(f\"Could not get documentation group for {cls}: {exc}\")\n",
      "/home/user/miniforge3/envs/torch/lib/python3.11/site-packages/gradio_client/documentation.py:106: UserWarning: Could not get documentation group for <class 'gradio.mix.Series'>: No known documentation group for module 'gradio.mix'\n",
      "  warnings.warn(f\"Could not get documentation group for {cls}: {exc}\")\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727148297.757869   30425 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1727148297.779019   31148 gl_context.cc:357] GL version: 3.1 (OpenGL ES 3.1 Mesa 23.2.1-1ubuntu3.1~22.04.2), renderer: D3D12 (AMD Radeon(TM) Graphics)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 3.40.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniforge3/envs/torch/lib/python3.11/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# Image Embedder 옵션 설정\n",
    "base_options = python.BaseOptions(model_asset_path='embedder.tflite')\n",
    "l2_normalize = True\n",
    "quantize = True\n",
    "options = vision.ImageEmbedderOptions(\n",
    "    base_options=base_options, l2_normalize=l2_normalize, quantize=quantize)\n",
    "\n",
    "# Image Embedder 생성\n",
    "embedder = vision.ImageEmbedder.create_from_options(options)\n",
    "\n",
    "# 유사도 계산 함수 정의\n",
    "def compare_images(image1, image2):\n",
    "    if image1 is None or image2 is None:\n",
    "        return \"이미지를 모두 업로드해주세요.\"\n",
    "    \n",
    "    mp_image1 = mp.Image(image_format=mp.ImageFormat.SRGB, data=image1)\n",
    "    mp_image2 = mp.Image(image_format=mp.ImageFormat.SRGB, data=image2)\n",
    "    \n",
    "    embedding_result1 = embedder.embed(mp_image1)\n",
    "    embedding_result2 = embedder.embed(mp_image2)\n",
    "    \n",
    "    similarity = vision.ImageEmbedder.cosine_similarity(\n",
    "        embedding_result1.embeddings[0],\n",
    "        embedding_result2.embeddings[0])\n",
    "    \n",
    "    return f\"두 이미지의 유사도: {similarity:.4f}\"\n",
    "\n",
    "# Gradio 인터페이스 생성\n",
    "iface = gr.Interface(\n",
    "    fn=compare_images,\n",
    "    inputs=[\n",
    "        gr.Image(type=\"numpy\", label=\"첫 번째 이미지 업로드\"),\n",
    "        gr.Image(type=\"numpy\", label=\"두 번째 이미지 업로드\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"이미지 유사도 비교\",\n",
    "    description=\"두 이미지를 업로드하여 유사도를 비교하세요.\"\n",
    ")\n",
    "\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e8a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756de7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
